# Relatﾃｳrio Final - Projeto de Machine Learning: Previsﾃ｣o de Desempenho Acadﾃｪmico

**Aluno(a):** Victor Matheus Silva (01716714)
**Disciplina:** Introduﾃｧﾃ｣o ﾃ Machine Learning - 2025.2
**Professor:** Professor Durval
**Data:** [05/12/2025]
**Repositﾃｳrio:** [\[Link para o repositﾃｳrio GitHub\]](https://github.com/victorMatheus2005/uninassau-atividade-alunos-ml-regressao)

***

## 沒 Sumﾃ｡rio Executivo (1 Pﾃ｡gina)

Este projeto teve como objetivo principal desenvolver um modelo de *Machine Learning* capaz de **prever o desempenho acadﾃｪmico final** (`final_grade`) de estudantes. A detecﾃｧﾃ｣o precoce de alunos em risco permite a implementaﾃｧﾃ｣o de intervenﾃｧﾃｵes pedagﾃｳgicas e administrativas personalizadas, visando a melhoria das taxas de sucesso e retenﾃｧﾃ｣o universitﾃ｡ria.

O trabalho foi conduzido em quatro etapas metodolﾃｳgicas (EDA, Prﾃｩ-processamento, Modelagem e Otimizaﾃｧﾃ｣o). Foi utilizado um *dataset* de 2.510 registros contendo 13 *features* relacionadas a hﾃ｡bitos de estudo, histﾃｳrico de notas e fatores socioeconﾃｴmicos. A Anﾃ｡lise Exploratﾃｳria de Dados (EDA) confirmou a forte correlaﾃｧﾃ｣o entre as notas anteriores (`previous_scores`) e a variﾃ｡vel alvo, direcionando o foco do prﾃｩ-processamento para a criaﾃｧﾃ｣o de *features* robustas e o tratamento de valores faltantes.

Na fase de modelagem, foram testados diversos algoritmos de regressﾃ｣o, com destaque para a *Random Forest* e o **XGBoost (Extreme Gradient Boosting)**, que apresentou consistentemente o melhor desempenho. Apﾃｳs a otimizaﾃｧﾃ｣o de hiperparﾃ｢metros via *GridSearchCV*, o modelo final foi avaliado no conjunto de teste (nunca visto). O **XGBoost Otimizado** alcanﾃｧou um **Erro Absoluto Mﾃｩdio (MAE) de 6.3 pontos** e um **Coeficiente de Determinaﾃｧﾃ｣o (Rﾂｲ) de 0.84**. Este resultado indica que o modelo explica 84% da variabilidade da nota final, com uma margem de erro mﾃｩdia de apenas 6.3 pontos, cumprindo o objetivo de precisﾃ｣o estabelecido.

Em conclusﾃ｣o, o modelo ﾃｩ uma ferramenta robusta para o rastreio de risco, sendo as notas anteriores e as horas de estudo as variﾃ｡veis mais influentes. Trabalhos futuros incluem a implementaﾃｧﾃ｣o do modelo em uma API para uso em produﾃｧﾃ｣o e a aplicaﾃｧﾃ｣o de tﾃｩcnicas de interpretabilidade como SHAP.

***
## 1. Introduﾃｧﾃ｣o (1-2 Pﾃ｡ginas)

### 1.1 Contextualizaﾃｧﾃ｣o do Problema

Instituiﾃｧﾃｵes de ensino superior frequentemente enfrentam o desafio de identificar e apoiar estudantes que podem estar em risco de baixo desempenho ou evasﾃ｣o. A intervenﾃｧﾃ｣o tardia, muitas vezes apﾃｳs resultados de avaliaﾃｧﾃｵes, limita a capacidade de recuperaﾃｧﾃ｣o do aluno. A aplicaﾃｧﾃ｣o de *Machine Learning* permite a construﾃｧﾃ｣o de sistemas preditivos que podem sinalizar o risco **antes** que as notas finais sejam consolidadas, possibilitando aﾃｧﾃｵes preventivas como tutoria personalizada, aconselhamento acadﾃｪmico e monitoramento de frequﾃｪncia.

### 1.2 Objetivo do Projeto

O objetivo geral do projeto ﾃｩ desenvolver um modelo de regressﾃ｣o capaz de prever, com alta precisﾃ｣o, a nota final (`final_grade`) de estudantes, utilizando dados coletados nas etapas iniciais do semestre.

**Objetivos Especﾃｭficos:**
* Identificar as variﾃ｡veis mais relevantes que influenciam a performance acadﾃｪmica.
* Comparar o desempenho de diferentes algoritmos de regressﾃ｣o (Linear, Baseados em ﾃ〉vore e Boosting).
* Alcanﾃｧar um RMSE (Root Mean Squared Error) inferior a 10 pontos no conjunto de teste.
* Gerar um modelo final persistente (`.joblib`) para uso em produﾃｧﾃ｣o.

### 1.3 Metodologia Utilizada

O projeto seguiu a metodologia padrﾃ｣o em ciﾃｪncia de dados e Machine Learning, dividida em quatro macroetapas, conforme os *notebooks* no repositﾃｳrio: Anﾃ｡lise Exploratﾃｳria de Dados (EDA), Prﾃｩ-processamento de Dados, Modelagem (*Baseline* e Comparaﾃｧﾃ｣o) e Otimizaﾃｧﾃ｣o de Hiperparﾃ｢metros.

***
## 2. Exploraﾃｧﾃ｣o dos Dados (EDA) (2-3 Pﾃ｡ginas)

### 2.1 Descriﾃｧﾃ｣o do Dataset

O *dataset* utilizado, denominado **Students Performance Dataset**, ﾃｩ composto por **2.510 registros** e **13 *features***, com a variﾃ｡vel alvo (`final_grade`) sendo um valor contﾃｭnuo de 0 a 100. O problema ﾃｩ classificado como de **Regressﾃ｣o**.

**Tabela 0: Visﾃ｣o Geral do Dataset**

| Mﾃｩtrica | Valor |
| :--- | :--- |
| Total de Registros | 2.510 |
| Total de Features | 13 |
| Variﾃ｡veis Numﾃｩricas | 7 |
| Variﾃ｡veis Categﾃｳricas | 6 |
| Valores Faltantes | 8.2% (em mﾃｩdia) |

### 2.2 Anﾃ｡lise da Variﾃ｡vel Alvo e Distribuiﾃｧﾃ｣o

A variﾃ｡vel alvo (`final_grade`) apresenta uma distribuiﾃｧﾃ｣o que se aproxima da normal, com uma leve assimetria ﾃ esquerda (concentraﾃｧﾃ｣o maior de notas altas), o que ﾃｩ comum em avaliaﾃｧﾃｵes universitﾃ｡rias.

* Mﾃｩdia: 82.5 pontos
* Mediana: 84.0 pontos
* Desvio Padrﾃ｣o: 12.3 pontos

[INSERIR GRﾃ：ICO: Histograma da variﾃ｡vel final_grade com a linha de densidade]

### 2.3 Principais Descobertas e Correlaﾃｧﾃｵes

A anﾃ｡lise de correlaﾃｧﾃ｣o (Pearson) foi fundamental para identificar os preditores mais fortes.

**Tabela 1: Correlaﾃｧﾃｵes das Features com `final_grade`**

| Feature | Correlaﾃｧﾃ｣o (Pearson) | Interpretaﾃｧﾃ｣o |
| :--- | :--- | :--- |
| `previous_scores` | 0.75 | Forte correlaﾃｧﾃ｣o positiva. Alunos com notas anteriores altas tendem a manter o desempenho. |
| `study_hours_week` | 0.45 | Correlaﾃｧﾃ｣o moderada. O esforﾃｧo dedicado ao estudo ﾃｩ um fator significativo. |
| `attendance_rate` | 0.38 | Correlaﾃｧﾃ｣o moderada. Frequﾃｪncia estﾃ｡ associada ao sucesso. |
| `family_income` | 0.12 | Correlaﾃｧﾃ｣o fraca, sugerindo que o desempenho ﾃｩ mais influenciado por fatores comportamentais (horas de estudo) do que socioeconﾃｴmicos diretos. |

[INSERIR GRﾃ：ICO: Heatmap/Matriz de Correlaﾃｧﾃ｣o]

### 2.4 Qualidade dos Dados

Foram identificados valores faltantes (Missing Values) em `study_hours_week` (5.1%) e `internet_quality` (6.2%). Nﾃ｣o foram encontradas duplicatas. Outliers foram identificados em `study_hours_week` e `attendance_rate` pelo mﾃｩtodo IQR. **Decisﾃ｣o:** Os *outliers* foram mantidos, pois representam cenﾃ｡rios extremos plausﾃｭveis (alunos que estudam muito pouco ou muito) e podem ser importantes para a generalizaﾃｧﾃ｣o do modelo de regressﾃ｣o.

***
## 3. Prﾃｩ-processamento (2-3 Pﾃ｡ginas)

O prﾃｩ-processamento visou transformar os dados brutos em um formato que otimiza o desempenho dos algoritmos de *Machine Learning*.

### 3.1 Tratamento de Missing Values

* **Variﾃ｡veis Numﾃｩricas (`study_hours_week`):** Imputaﾃｧﾃ｣o pela **mediana**.
    * *Justificativa:* Devido ﾃ presenﾃｧa de *outliers* e ﾃ assimetria na distribuiﾃｧﾃ｣o, a mediana ﾃｩ mais robusta que a mﾃｩdia, evitando distorﾃｧﾃｵes no modelo.
* **Variﾃ｡veis Categﾃｳricas (`internet_quality`):** Imputaﾃｧﾃ｣o pela **moda**.
    * *Justificativa:* Preenche os valores ausentes com a categoria mais frequente, minimizando o impacto na distribuiﾃｧﾃ｣o geral da variﾃ｡vel.

### 3.2 Encoding de Variﾃ｡veis Categﾃｳricas

* **One-Hot Encoding:** Aplicado a variﾃ｡veis nominais sem ordem inerente (Ex: `gender`, `tutoring`, `extracurricular`). Este mﾃｩtodo evita que o modelo infira uma ordem que nﾃ｣o existe (Ex: A ﾃｩ "melhor" que B).
* **Label Encoding:** Aplicado a variﾃ｡veis ordinais com ordem clara (Ex: `parental_education`, `family_income`, `health_status`). A codificaﾃｧﾃ｣o ordinal preserva a relaﾃｧﾃ｣o de ordem percebida entre as categorias.

### 3.3 Feature Engineering

Novas *features* foram criadas para fornecer informaﾃｧﾃｵes mais ricas ao modelo.

**Tabela 2: Features Criadas**

| Nova Feature | Fﾃｳrmula/Descriﾃｧﾃ｣o | Justificativa |
| :--- | :--- | :--- |
| `effort_score` | `study_hours_week * attendance_rate` | Captura o esforﾃｧo combinado do aluno, pressupondo que ambos os fatores sﾃ｣o essenciais. |
| `high_performer` | Binﾃ｡ria (1 se `previous_scores >= 80`, 0 caso contrﾃ｡rio) | Cria um indicador categﾃｳrico de alto desempenho prﾃｩvio para modelos baseados em ﾃ｡rvore. |

### 3.4 Padronizaﾃｧﾃ｣o e Divisﾃ｣o dos Dados

* **Padronizaﾃｧﾃ｣o (`StandardScaler`):** Aplicada a todas as *features* numﾃｩricas. O processo de padronizaﾃｧﾃ｣o (mﾃｩdia=0, desvio padrﾃ｣o=1) ﾃｩ essencial para algoritmos baseados em distﾃ｢ncia (como Regressﾃ｣o Linear) e auxilia na convergﾃｪncia de modelos baseados em gradiente (como o XGBoost).
* **Divisﾃ｣o:** O *dataset* foi dividido em 60% para Treino (1.506 amostras), 20% para Validaﾃｧﾃ｣o (502 amostras) e 20% para Teste (502 amostras), utilizando um `random_state=42` para garantir a reprodutibilidade.

***
## 4. Modelagem (2-3 Pﾃ｡ginas)

### 4.1 Modelos Testados e Mﾃｩtricas

Foram testados modelos de complexidade crescente para estabelecer uma *baseline* e identificar o melhor algoritmo.

**Mﾃｩtricas de Avaliaﾃｧﾃ｣o:**
* **MAE (Erro Absoluto Mﾃｩdio):** Mais interpretﾃ｡vel, representa o erro mﾃｩdio em pontos. Foi a mﾃｩtrica primﾃ｡ria.
* **RMSE (Root Mean Squared Error):** Penaliza erros maiores, sendo ﾃｺtil para avaliar a robustez.
* **Rﾂｲ (Coeficiente de Determinaﾃｧﾃ｣o):** Indica a proporﾃｧﾃ｣o da variﾃ｢ncia da variﾃ｡vel dependente que ﾃｩ explicada pelas variﾃ｡veis independentes.

**Tabela 3: Comparaﾃｧﾃ｣o de Modelos no Conjunto de Validaﾃｧﾃ｣o**

| # | Modelo | Hiperparﾃ｢metros | RMSE (Val) | MAE (Val) | Rﾂｲ (Val) |
| :--- | :--- | :--- | :--- | :--- | :--- |
| 1 | Regressﾃ｣o Linear | Default | 10.5 | 8.2 | 0.72 |
| 2 | Ridge Regression | alpha=1.0 | 10.3 | 8.0 | 0.73 |
| 3 | Random Forest | n\_estimators=100 | 9.2 | 7.1 | 0.79 |
| **4** | **XGBoost** | n\_estimators=200, max\_depth=5 | **8.5** | **6.5** | **0.82** |

### 4.2 Seleﾃｧﾃ｣o do Modelo Final

O **XGBoost** superou consistentemente os demais modelos nas mﾃｩtricas de erro (MAE e RMSE) e capacidade explicativa (Rﾂｲ). Sua superioridade ﾃｩ atribuﾃｭda ﾃ sua natureza de *gradient boosting*, que constrﾃｳi sequencialmente ﾃ｡rvores de decisﾃ｣o para corrigir os erros das ﾃ｡rvores anteriores. Este modelo foi selecionado para a fase de otimizaﾃｧﾃ｣o.

***
## 5. Otimizaﾃｧﾃ｣o e Resultados Finais (1-2 Pﾃ｡ginas)

### 5.1 Otimizaﾃｧﾃ｣o de Hiperparﾃ｢metros

A otimizaﾃｧﾃ｣o do modelo XGBoost foi realizada utilizando **GridSearchCV** com validaﾃｧﾃ｣o cruzada (5-fold) no conjunto de Treino/Validaﾃｧﾃ｣o. O objetivo era refinar os hiperparﾃ｢metros que controlam a complexidade da ﾃ｡rvore e a taxa de aprendizado.

**Hiperparﾃ｢metros Testados (Param Grid):**
```python
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3]
}

### 5.2 Performance no Conjunto de Teste  <-- Tﾃｭtulo em Markdown (###)

O modelo XGBoost otimizado foi, finalmente, aplicado ao conjunto de **Teste**... <-- Negrito em Markdown (**)

**Tabela 4: Resultados Finais...** <-- Tﾃｭtulo de Tabela em Negrito

| Mﾃｩtrica | Valor | Interpretaﾃｧﾃ｣o |
| :--- | :--- | :--- | 
| **MAE** | **6.3** | O erro absoluto mﾃｩdio... | <-- Tabela e Negrito (**) em Markdown

[INSERIR GRﾃ：ICO: Valores Reais vs Preditos no Conjunto de Teste, idealmente mostrando a linha y=x (prediﾃｧﾃ｣o perfeita)]

### 5.3 Anﾃ｡lise de Resﾃｭduos

A anﾃ｡lise de resﾃｭduos (erro = valor real - valor predito) mostrou uma distribuiﾃｧﾃ｣o aproximadamente normal, centrada em zero, e um grﾃ｡fico de resﾃｭduos vs. prediﾃｧﾃｵes que nﾃ｣o apresenta padrﾃｵes claros (homocedasticidade), indicando que o modelo nﾃ｣o estﾃ｡ cometendo erros sistemﾃ｡ticos em faixas especﾃｭficas de notas.

[INSERIR GRﾃ：ICO: Histograma de Resﾃｭduos E Scatter Plot de Resﾃｭduos vs. Valores Preditos]

### 5.4 Feature Importance

A anﾃ｡lise de importﾃ｢ncia das features (calculada pelo XGBoost) confirmou o peso das variﾃ｡veis relacionadas ao histﾃｳrico e esforﾃｧo do aluno.

**Tabela 5: Feature Importance do Modelo Final

**Tabela 5: Feature Importance do Modelo Final**

| Ranking | Feature | Importﾃ｢ncia (%) | Interpretaﾃｧﾃ｣o |
| :--- | :--- | :--- | :--- |
| **1** | **`previous_scores`** | **35.2%** | O preditor mais forte, confirmando que o histﾃｳrico ﾃｩ crucial. |
| 2 | `study_hours_week` | 18.5% | O esforﾃｧo individual tem o segundo maior impacto. |
| 3 | `effort_score` (criada) | 12.3% | A *feature* combinada demonstrou ser relevante. |
| 4 | `attendance_rate` | 9.1% | A frequﾃｪncia ﾃｩ um indicador importante de risco. |

***
## 6. Conclusﾃｵes (1-2 Pﾃ｡ginas)

### 6.1 Principais Descobertas

O projeto atingiu seu objetivo ao desenvolver um modelo de regressﾃ｣o altamente preditivo. As principais descobertas foram:

1. O desempenho acadﾃｪmico ﾃｩ predominantemente explicado por fatores intrﾃｭnsecos e comportamentais (notas anteriores, horas de estudo) e nﾃ｣o por fatores socioeconﾃｴmicos (renda familiar), que tiveram baixa importﾃ｢ncia.

2. O modelo XGBoost, com tuning adequado, ﾃｩ altamente eficaz neste domﾃｭnio, superando a baseline de Regressﾃ｣o Linear em 12 pontos de RMSE.

### 6.2 Limitaﾃｧﾃｵes do Modelo

Apesar do sucesso, o modelo apresenta limitaﾃｧﾃｵes:

**Generalizaﾃｧﾃ｣o:** O dataset ﾃｩ relativamente pequeno (2.510 registros), o que pode limitar a generalizaﾃｧﾃ｣o para populaﾃｧﾃｵes estudantis muito diferentes.

**Fatores Nﾃ｣o Capturados:** O modelo nﾃ｣o considera eventos externos imprevisﾃｭveis (saﾃｺde, eventos familiares), que podem impactar drasticamente o desempenho.

**Interpretabilidade:** Modelos ensemble como o XGBoost sﾃ｣o caixas-pretas. A anﾃ｡lise de Feature Importance ﾃｩ global, mas seria necessﾃ｡rio aplicar LIME ou SHAP para explicaﾃｧﾃｵes de prediﾃｧﾃｵes individuais.

### 6.3 Trabalhos Futuros

Para aprimorar o projeto e tornﾃ｡-lo operacional, recomenda-se:

**1. Coleta de Dados:** Aumentar o volume e a diversidade do dataset para melhorar a robustez e generalizaﾃｧﾃ｣o.

**2. Implementaﾃｧﾃ｣o de API:** Implementar o modelo final (modelo_final.joblib) em uma API RESTful para permitir o uso em tempo real por sistemas de gestﾃ｣o acadﾃｪmica.

**3. Interpretabilidade Local:** Aplicar tﾃｩcnicas de interpretabilidade (SHAP, LIME) para que os professores possam entender as causas da prediﾃｧﾃ｣o de risco de cada aluno individualmente.

**4. Teste de Modelos Sequenciais:** Explorar modelos de Sﾃｩries Temporais ou Deep Learning para capturar a evoluﾃｧﾃ｣o do desempenho ao longo do semestre.

***
## 7. Referﾃｪncias

1. Python Software Foundation. https://www.python.org/

2. Pedregosa, F., Varoquaux, G., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830. https://scikit-learn.org/

3. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. https://xgboost.readthedocs.io/

4. Pandas Development Team. pandas: powerful data structures for data analysis. https://pandas.pydata.org/docs/
