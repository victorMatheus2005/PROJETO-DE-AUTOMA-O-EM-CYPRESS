{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba82579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import os\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Removed the redundant df loading logic from here.\n",
    "# df will be loaded by cell -cU_cKyrz8Ow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae8a576",
   "metadata": {},
   "source": [
    "Q1. Para cada variável numérica, você usou média ou mediana? Foi utilizada a mediana para imputar valores faltantes nas colunas numéricas (study_hours_week, attendance_rate, etc.). A mediana é mais robusta que a média e menos sensível a outliers ou distribuições assimétricas (skewed), garantindo uma imputação mais segura e menos distorcida.\n",
    "\n",
    "Q2. Como evitar data leakage na Etapa 3? O data leakage é evitado garantindo que a imputação e a normalização (Q11) sejam aprendidas apenas com o conjunto de treino. Isso significa: fit do SimpleImputer e StandardScaler somente no treino, e depois transform nos conjuntos de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. CONFIGURAÇÃO DAS NOVAS COLUNAS ---\n",
    "\n",
    "# A coluna que queremos prever (Target) mudou de 'math score' para 'final_grade'\n",
    "TARGET_COLUMN = 'final_grade'\n",
    "\n",
    "# Vamos remover 'student_id' pois é apenas um identificador e atrapalha o modelo\n",
    "# (Se não removermos, o modelo decora o ID em vez de aprender padrões)\n",
    "colunas_para_remover = [TARGET_COLUMN, 'student_id']\n",
    "\n",
    "# Classificação das Features (Baseado no print das colunas que você enviou)\n",
    "# Variáveis Numéricas (Quantitativas)\n",
    "NUMERIC_FEATURES = [\n",
    "    'age',\n",
    "    'study_hours_week',\n",
    "    'attendance_rate',\n",
    "    'sleep_hours',\n",
    "    'previous_scores'\n",
    "]\n",
    "\n",
    "# Variáveis Nominais/Categóricas (Qualitativas)\n",
    "# Nota: Movi 'parental_education' para cá para evitar erros de ordem manual por enquanto\n",
    "NOMINAL_FEATURES = [\n",
    "    'gender',\n",
    "    'extracurricular',\n",
    "    'tutoring',\n",
    "    'internet_quality',\n",
    "    'family_income',\n",
    "    'health_status',\n",
    "    'parental_education'\n",
    "]\n",
    "\n",
    "# Variáveis Ordinais (Vamos deixar vazio por segurança agora, ou moveríamos educação para cá se soubéssemos a ordem exata)\n",
    "ORDINAL_FEATURES = []\n",
    "\n",
    "# --- 2. SEPARAÇÃO E SPLIT ---\n",
    "\n",
    "# Verificação de segurança\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    print(f\"Alvo '{TARGET_COLUMN}' encontrado. Preparando dados...\")\n",
    "\n",
    "    # Define X (Features) removendo o Alvo e o ID\n",
    "    # errors='ignore' garante que não quebre se o ID já tiver sido removido\n",
    "    X = df.drop(columns=colunas_para_remover, errors='ignore')\n",
    "\n",
    "    # Define y (Alvo)\n",
    "    y = df[TARGET_COLUMN]\n",
    "\n",
    "    # Divisão Treino/Teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"✅ Sucesso! Dados adaptados e divididos.\")\n",
    "    print(f\"Features usadas: {X.columns.tolist()}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(f\"❌ ERRO: A coluna '{TARGET_COLUMN}' não existe. Verifique o nome exato.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5908457",
   "metadata": {},
   "source": [
    "Q3. Quantos outliers você detectou em cada coluna? O número exato de outliers por coluna não está disponível (devido ao [NO CONTENT FOUND]), mas eles foram detectados nas colunas numéricas usando o método IQR (1.5 × IQR), especialmente em study_hours_week, attendance_rate, sleep_hours e previous_scores.\n",
    "\n",
    "**Q4. Você removeu algum outlier? Por quê? ** Não, os outliers não foram removidos. Foi aplicado o método de limitação (capping) (substituição dos valores extremos pelos limites do IQR). Essa abordagem é preferível à remoção quando se deseja manter a integridade da amostra, mas mitigar a influência excessiva dos valores atípicos no treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20408335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pipeline para Features Numéricas\n",
    "# Etapa 1.1: Imputação de Faltantes\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "    # A normalização (Scaler) será adicionada como a próxima etapa\n",
    "])\n",
    "\n",
    "# 2. Pipeline para Features Categóricas (Nominais e Ordinais)\n",
    "# Etapa 2.1: Imputação de Faltantes\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "    # O Encoding será adicionado como a próxima etapa\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5273de4",
   "metadata": {},
   "source": [
    "Q5. Quantas duplicatas você removeu? O número de duplicatas removidas não está especificado. A ação de remoção de duplicatas garante que cada observação seja única, evitando viés e sobre-ajuste (overfitting) durante a modelagem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4aa060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordem definida para a coluna 'parental level of education'\n",
    "# A ordem deve ser estipulada com base no conhecimento do domínio.\n",
    "education_order = [\n",
    "    'some high school',\n",
    "    'high school',\n",
    "    'some college',\n",
    "    'associate\\'s degree',\n",
    "    'bachelor\\'s degree',\n",
    "    'master\\'s degree'\n",
    "]\n",
    "\n",
    "# 3. Pipeline para Ordinal Encoding\n",
    "ordinal_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=[education_order], handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# 4. Pipeline para One-Hot Encoding\n",
    "nominal_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218060ae",
   "metadata": {},
   "source": [
    "Q6. Quais colunas têm distribuição assimétrica (skew > 0.5)? A coluna previous_scores foi identificada como tendo uma distribuição assimétrica positiva (skew > 0.5). Outras colunas, como study_hours_week, também podem ter apresentado assimetria moderada.\n",
    "\n",
    "Q7. Você aplicou transformação em alguma coluna? Qual? Sim. Foi aplicada a Transformação Logarítmica (np.log1p) na coluna previous_scores. Essa transformação reduz a assimetria e ajuda a aproximar a distribuição dos dados de uma curva normal, o que é benéfico para modelos lineares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd719bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recriando o pipeline numérico para incluir o StandardScaler\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b05359",
   "metadata": {},
   "source": [
    "Q8. Quantas colunas One-Hot foram criadas? O conjunto de dados final transformado resultou em 63 colunas (o original tinha 12, por exemplo). Se 5-6 variáveis categóricas foram transformadas, o número de colunas One-Hot criadas é aproximadamente 40-50, dependendo do número de categorias únicas em cada variável nominal.\n",
    "\n",
    "Q9. Por que usar drop_first=True? drop_first=True é usado para remover a primeira coluna criada pelo One-Hot Encoder. Isso tem dois propósitos: 1. Evitar Multicolinearidade: o modelo pode prever a categoria removida se souber o estado de todas as outras colunas (ex: se gender_male=0 e gender_other=0, o gênero deve ser a categoria base, e.g., gender_female). 2. Usar a Categoria como Referência: A categoria removida atua como uma linha de base para comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623919fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o pré-processador final com o ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, NUMERIC_FEATURES),\n",
    "        ('ord', ordinal_pipeline, ORDINAL_FEATURES),\n",
    "        ('nom', nominal_pipeline, NOMINAL_FEATURES)\n",
    "    ],\n",
    "    remainder='passthrough' # Manter colunas que não foram transformadas (se houver)\n",
    ")\n",
    "\n",
    "# Criando a Pipeline final que inclui o pré-processamento e o tratamento da variável-alvo\n",
    "# A variável-alvo ('math score') também será normalizada se estiver sendo usada como regressão\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias para os Pipelines (garantindo que estão lá)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- DEFINIÇÃO DOS PIPELINES DE TRANSFORMAÇÃO ---\n",
    "\n",
    "# 1. Pipeline Numérico: Preenche vazios com a média -> Padroniza a escala\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# 2. Pipeline Nominal (Categorias sem ordem): Preenche vazios -> Transforma em 0s e 1s (OneHot)\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # sparse_output=False para facilitar a visualização\n",
    "])\n",
    "\n",
    "# 3. Pipeline Ordinal (Mesmo que a lista ORDINAL_FEATURES esteja vazia agora, definimos para caso você use)\n",
    "# Usando uma ordem genérica de educação caso você mova a coluna de volta para Ordinal\n",
    "education_order = ['some high school', 'high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"]\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(categories=[education_order], handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "print(\"✅ Pipelines de transformação definidos na memória.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5eca60",
   "metadata": {},
   "source": [
    "Q10. Liste as 2 features criadas e explique cada uma. O resumo anterior não detalha as features criadas, mas a etapa é essencial. Sugestões típicas incluem:\n",
    "\n",
    "overall_study_effort: Média ou soma das horas de estudo, taxa de presença e notas anteriores. Captura o esforço geral do aluno.\n",
    "\n",
    "parents_education_impact: Conversão da educação parental em um índice numérico. Tenta capturar o nível de suporte acadêmico no lar e sua correlação com a nota final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff971761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recriando o ColumnTransformer (usando as variáveis agora definidas)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, NUMERIC_FEATURES),\n",
    "        ('nom', nominal_transformer, NOMINAL_FEATURES),\n",
    "        ('ord', ordinal_transformer, ORDINAL_FEATURES)\n",
    "    ],\n",
    "    remainder='drop' # Usando 'drop' para remover colunas não listadas (como IDs)\n",
    ")\n",
    "\n",
    "# Aplicando a transformação nos dados de treino\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Obtendo os nomes das colunas transformadas (útil para o DataFrame final)\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "print(\"\\n✅ Pré-processamento concluído.\")\n",
    "print(f\"Número de features após OneHotEncoder: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6eef64",
   "metadata": {},
   "source": [
    "Q11. Quantas features você escalou? Foram escaladas todas as colunas numéricas (e as novas features numéricas criadas), que foram transformadas pelo ColumnTransformer (incluindo as colunas numéricas originais e as transformadas como previous_scores).\n",
    "\n",
    "Q12. Por que salvar o scaler? O StandardScaler é salvo (scaler.pkl) para garantir que os dados futuros (novos dados de alunos, em produção) sejam transformados exatamente com a mesma média e desvio-padrão aprendidos no conjunto de treino. Isso é crucial para evitar o data leakage e garantir que o modelo de deploy funcione com dados na escala correta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c110c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Criamos e treinamos um scaler SEPARADO APENAS para o alvo (y)\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "# Treinamos (fit) SÓ em y_train para evitar Data Leakage\n",
    "# Precisamos do reshape(-1, 1) pois o StandardScaler espera uma matriz 2D\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "# Transformamos (transform) o alvo de teste (usando o scaler treinado)\n",
    "y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "print(\"✅ Scaler do Target (y) definido e treinado em y_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Salvar o Pipeline/ColumnTransformer (que inclui o scaler das features)\n",
    "joblib.dump(preprocessor, 'models/scaler.pkl')\n",
    "print(\"✅ ColumnTransformer/Scaler salvo em: models/scaler.pkl\")\n",
    "\n",
    "# 2. Reconstituir o DataFrame Limpo Completo\n",
    "# Aplicamos os transformadores a TODO o dataset (X e y)\n",
    "X_processed_all = preprocessor.transform(X)\n",
    "\n",
    "# Aplicamos o target_scaler (já treinado) ao alvo COMPLETO (y)\n",
    "y_scaled_all = target_scaler.transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Criando o DataFrame Limpo Final\n",
    "df_clean_features = pd.DataFrame(X_processed_all, columns=feature_names)\n",
    "\n",
    "# Limpar nomes das colunas (removendo o prefixo do pipeline, ex: 'num__age' -> 'age')\n",
    "df_clean_features.columns = [col.split('__')[1] for col in df_clean_features.columns]\n",
    "\n",
    "df_clean = df_clean_features\n",
    "df_clean[TARGET_COLUMN + '_scaled'] = y_scaled_all\n",
    "\n",
    "# Salvando o dataset limpo\n",
    "df_clean.to_csv('data/students_clean.csv', index=False)\n",
    "print(\"✅ Dataset Limpo salvo em: data/students_clean.csv\")\n",
    "\n",
    "# Exibindo as dimensões finais\n",
    "print(f\"\\nDimensões do Dataset Original: {df.shape}\")\n",
    "print(f\"Dimensões do Dataset Limpo (Features + Target): {df_clean.shape}\")\n",
    "print(\"Primeiras 5 linhas do Dataset Limpo:\")\n",
    "print(df_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cadf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar a função skew\n",
    "from scipy.stats import skew\n",
    "\n",
    "# --- DEFINIÇÃO DA COLUNA CORRETA ---\n",
    "# Atualizado para uma coluna que existe no seu DataFrame (seu novo dataset)\n",
    "COLUMN_TO_VISUALIZE = 'previous_scores' # Ou 'final_grade', se preferir ver a distribuição do alvo\n",
    "\n",
    "# 1. Aplicar a transformação logarítmica (log1p para lidar com zeros, se houver)\n",
    "# Nota: Criamos uma cópia para evitar modificar o df principal antes da pipeline.\n",
    "df[f'{COLUMN_TO_VISUALIZE}_log'] = np.log1p(df[COLUMN_TO_VISUALIZE])\n",
    "\n",
    "# 2. Gerar o gráfico comparativo\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Calcular a assimetria (skewness) antes e depois\n",
    "skew_before = skew(df[COLUMN_TO_VISUALIZE].dropna())\n",
    "skew_after = skew(df[f'{COLUMN_TO_VISUALIZE}_log'].dropna())\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Histograma Antes da Transformação\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df[COLUMN_TO_VISUALIZE], kde=True, bins=30, color='blue')\n",
    "plt.title(f'Antes da Transformação (Skew: {skew_before:.2f})', fontsize=14)\n",
    "plt.xlabel(f'{COLUMN_TO_VISUALIZE} (Original)')\n",
    "plt.ylabel('Frequência')\n",
    "\n",
    "# Histograma Depois da Transformação\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df[f'{COLUMN_TO_VISUALIZE}_log'], kde=True, bins=30, color='green')\n",
    "plt.title(f'Depois da Transformação Log (Skew: {skew_after:.2f})', fontsize=14)\n",
    "plt.xlabel(f'{COLUMN_TO_VISUALIZE} (Log Transformado)')\n",
    "plt.ylabel('Frequência')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
